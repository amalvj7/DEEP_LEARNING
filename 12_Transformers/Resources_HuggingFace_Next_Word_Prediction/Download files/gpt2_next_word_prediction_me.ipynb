{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f2cf72-e069-4151-9d9a-5da3d893e6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f7a92bccd04f388b50e259cf70e0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amal\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amal\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Amal\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d45cbad07264473b0a12da28f3a8315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848b37c140294a619a6cbf7acf05df07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b492d228454e496b9391859248707d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2812fec9ffcd424d995843a243a3aa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20854a57bab144d3a296dcec0c6afaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef150f20701645bfa39f2befb746d8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, so that's a hard one, but let's see how we can achieve it!\\n\\nYou can learn more about this topic in this talk â€” a lot of that is code, but there's also a lot of code that goes into how to write your own model, so let's look at how to solve for that.\\n\\nYou'll see that you can write your own models, with the following syntax:\\n\\nmodel() {\\n\\nreturn (\\n\\n(\\n\\n<Text>\\n\\n<Alias>\\n\\n(\\n\\n<Text>\\n\\n<Alias>\\n\\n(\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n<Alias>\\n\\n</Alias>\\n\\n));\\n\\n}\\n\\nThen you can write your own model, and then you can write your own model from that.\\n\\nThe last part of that is code. You'll see that we can write our own (and with more) model for the following:\\n\\nimport std.model as model;\\n\\nThis is one of the things that I like about this\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an ontology. I don't want to write an ontology of any sort. I want a language that can read a lot of data and understand what it's doing. So, I'm not saying that you can't write a language with a language model.\\n\\nBut I'll give you a little bit of an example. What we have is a language that does a lot of reading, but it is hard to read a lot of data. So, it's possible to just write a language that can read a lot of data. I mean, I'm not saying that you can write a language that can access a lot of data, but I'm saying that you can write a language that can read a lot of data. And, by the way, I'm not saying that you can write a language that can read a lot of data, but I'm saying that you can write a language that can read a lot of data. So, this is a language that can read a lot of data and understand what they're doing.\\n\\nAnd this is a language that can read a lot of data, but, by the way, I'm not saying that you can do that. But I'm saying that you can do that.\\n\\nAnd this\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't have a whole lot of experience with languages and I don't know much about them. I'm really curious about what they're like, what they're like in general, and what they do differently. I think they all have their own things that they do differently, like, they're very general, their thing is to be able to understand a certain language better. I think that's why I'm here today and I'm very curious about what they're like. I'm really curious about what they're like in general and what they do differently.\\n\\nLINKS\\n\\nIt's definitely a great place to talk about the language, it's a really fun place to be a speaker of languages, and I'd love to get some feedback about what they're like and how you can help.\\n\\nI'm excited to be in the classroom and in this community. I am a very excited person to be a speaker! I am excited to be in the classroom and in this community. I am a very excited person to be a speaker! I am a very excited person to be a speaker!\\n\\nThank you for taking time to speak to us today, I'm so excited to be there.\\n\\nThank you for taking time to speak\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I'm not a language model. I'm a model of programming languages. Let's talk about what I am.\\n\\nA language is a set of rules for the behavior of a program.\\n\\nThe first rule is that no program should ever be terminated.\\n\\nThe second rule is that no program should be terminated.\\n\\nThe third rule is that no program should ever be terminated.\\n\\nThe fourth rule is that no program should ever be terminated.\\n\\nThe last rule is that no program should ever be terminated.\\n\\nThe fifth rule is that no program should ever be terminated.\\n\\nWhen I first entered Haskell, I was not able to see the meaning of the first rule. I was not able to understand the third rule. I was only able to see the fourth rule.\\n\\nThis is because what I had seen is not a good program, it's not a good program, it's not a good program.\\n\\nI was not able to understand the third rule. I was not able to understand the fourth rule.\\n\\nI was told that they had to make sure that we were clear with the fourth rule. I was not told that they were clear with the fifth.\\n\\nBut that is not enough. There is\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, which means I\\'m not in any way a \"todoist\" and are really just a user. I\\'m just a person who is really interested in things besides language. I\\'m looking for people who want to learn languages and I\\'m hoping to get it out of the way. I\\'m writing this document to show you how you can help me translate it into English.\\n\\nThis is an original document. I\\'m just going to show you how to get it out of the way.\\n\\nGo to your browser in the tab menu at the top of the page and choose \"Translate\".\\n\\nClick on \"OK\" and choose \"OK\"\\n\\nYou should see this:\\n\\nYou should see this text:\\n\\nHello, I\\'m a language model, which means I\\'m not in any way a \"todoist\" and are really just a user. I\\'m looking for people who want to learn languages and I\\'m hoping to get it out of the way. I\\'m writing this document to show you how you can help me translate it into English.\\n\\nThis is an original document. I\\'m just going to show you how to get it out of the way.\\n\\nGo to your browser in the tab menu at the top'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78591485-30df-4597-80c0-2bf390987573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.1629, -0.2166, -0.1410,  ..., -0.2619, -0.0819,  0.0092],\n",
       "         [ 0.4628,  0.0248, -0.0785,  ..., -0.0859,  0.5122, -0.3939],\n",
       "         [-0.0644,  0.1551, -0.6306,  ...,  0.2488,  0.3691,  0.0833],\n",
       "         ...,\n",
       "         [-0.5591, -0.4490, -1.4540,  ...,  0.1650, -0.1302, -0.3740],\n",
       "         [ 0.1400, -0.3875, -0.7916,  ..., -0.1780,  0.1824,  0.2185],\n",
       "         [ 0.1721, -0.2420, -0.1124,  ..., -0.1068,  0.1205, -0.3213]]],\n",
       "       grad_fn=<ViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9132d260-3b82-408f-b2b4-0bda9222f06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b33efad4e461e89298aeca1a87e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amal\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amal\\.cache\\huggingface\\hub\\models--roberta-base-openai-detector. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c96495f7754b8688cb14edf4f9ce69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a36911b3c0483489404e291d086979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964124791fd44ee8a86654da6a1ee75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68eb546de3f6468497ff776fd7437143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43414b2bf0444b9499af79fed424769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Real', 'score': 0.8036574721336365}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"roberta-base-openai-detector\")\n",
    "print(pipe(\"Hello world! Is this content AI-generated?\"))  # [{'label': 'Real', 'score': 0.8036582469940186}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fc53b-08f4-47e8-bd1f-a82d42a260bf",
   "metadata": {},
   "source": [
    "### Transformers pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff42150-77d7-4137-956c-fea4b5df95a3",
   "metadata": {},
   "source": [
    "### GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc6bbb6-96d1-45d2-9f4c-c4a2f102c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed510f3a-8a06-42ba-a701-394d1189968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
