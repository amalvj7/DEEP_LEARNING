{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS_V6StcOXOr"
   },
   "source": [
    "# Problem Statement: **Hyperparameter Tuning for AtliQâ€™s Fashion Item Classifier**\n",
    "\n",
    "### AtliQ Fashion wants to develop a neural network to classify fashion items using the FashionMNIST dataset. Your task is to optimize the neural network's performance by fine-tuning its hyperparameters. We will be using **FashionMNIST** dataset but since the dataset is large, we will work with only a subset to ensure that the solution is computationally feasible.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* transforms.Compose (PyTorch): [Link](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)\n",
    "* Optuna (Hyperparameter Optimization Framework) [Link](https://optuna.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mz3pMHtjJ51"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Nv9L_r8yOjoo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65TpZ6z3YVcT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrkVANZOnea"
   },
   "source": [
    "**Dataset Overview**\n",
    "\n",
    "* Dataset: FashionMNIST\n",
    "* Classes: 10 (e.g., T-shirts, trousers, shoes)\n",
    "* Training Images: Subset of 10,000 (randomly sampled from 60,000)\n",
    "* Test Images: Subset of 2,000 (randomly sampled from 10,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJiS4LzoYWh9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caIjvjTMOuKb"
   },
   "source": [
    "**Step1**: Load and Sample the Dataset\n",
    "\n",
    "* Load the FashionMNIST dataset using torchvision.datasets.\n",
    "* Sample 10,000 images for training and 2,000 images for testing.\n",
    "* Normalize the pixel values to the range [-1, 1].\n",
    "* Create PyTorch DataLoaders for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4ESYRM7JK-UW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform: Normalize and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Centers the pixel values around 0 and scales them to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "\n",
    "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Sample the datset\n",
    "\n",
    "train_subset_size = 10000\n",
    "test_subset_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9NJ-ejsYYU7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbAUaSPmSfsZ"
   },
   "source": [
    "**Step2**: Create Dataloaders\n",
    "\n",
    "* batch size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xPPkBtqbacNM"
   },
   "outputs": [],
   "source": [
    "train_subset, _ = random_split(dataset, [train_subset_size, len(dataset) - train_subset_size])\n",
    "test_subset, _ = random_split(test_dataset, [test_subset_size, len(test_dataset) - test_subset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Hz00fg0sSqT2"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset , batch_size = batch_size , shuffle = True)\n",
    "test_loader = DataLoader(test_subset , batch_size = batch_size , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oq15WEfhS1HV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 10000\n",
      "Testing data size: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(train_subset)}\")\n",
    "print(f\"Testing data size: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dSLUp9EYaG9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGRSPjG0StuU"
   },
   "source": [
    "**Step3**: Define the Neural Network\n",
    "\n",
    "* Create a fully connected feed-forward neural network (no CNN).\n",
    "\n",
    "Structure:\n",
    "* Input layer: 784 neurons (28x28 image flattened).\n",
    "* 1st hidden layer: 128 neurons with ReLU activation.\n",
    "* 2nd hidden layer: 64 neurons with ReLU activation.\n",
    "* Output layer: 10 neurons (one for each class) with Softmax activation.\n",
    "\n",
    "Use `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QJYOKAZmTpkb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNN(\n",
      "  (network): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (6): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FashionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784 , 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear( 128 ,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64 , 10),\n",
    "            nn.Softmax(dim = 1)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = FashionNN()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJmkGAINYb0C"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNO5O1W_UI7L"
   },
   "source": [
    "**Step 3**: Train the Base Model\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Set the following base hyperparameters:\n",
    "* Loss function: Cross Entropy Loss\n",
    "* Learning rate: 0.01\n",
    "* Batch size: 32\n",
    "* Optimizer: SGD\n",
    "* Epochs: 100\n",
    "\n",
    "Train the model and record the training/validation accuracy and loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb5gFuiUUTDC"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters() , lr = 0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        loss = loss_function(predictions , labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAu40Az5itQY"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZBFzIaZU3gU"
   },
   "source": [
    "**Step 4**: Perform Hyperparameter Tuning\n",
    "Instructions:\n",
    "\n",
    "**Grid Search:**\n",
    "\n",
    "Hyperparameters:\n",
    "* Learning rate: [0.001, 0.01, 0.1]\n",
    "* Batch size: [32, 64]\n",
    "* Evaluate all combinations systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TcJybSHKVSu1"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (659348959.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    optimizer = # Code Here\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define grid search parameters\n",
    "learning_rates =[0.001, 0.01, 0.1]\n",
    "batch_sizes =[32, 64]\n",
    "\n",
    "# Train and evaluate for all combinations\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        optimizer = optim(model.parameters() , lr = lr)\n",
    "        train_loader = DataLoader(batch\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        print(f\"LR: {lr}, Batch size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_params = {'lr': lr, 'batch_size': batch_size}\n",
    "\n",
    "print(f\"Best Params (Grid Search): {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-oxYMQFiuwX"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZBDWk_wV3hB"
   },
   "source": [
    "**Random Search:**\n",
    "\n",
    "Randomly select hyperparameters for 5 trials from:\n",
    "* Learning rate: [0.0001, 0.001, 0.01, 0.1]\n",
    "* Batch size: [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpbk9NjpV5GI"
   },
   "outputs": [],
   "source": [
    "# Define random search space\n",
    "learning_rates =\n",
    "batch_sizes =\n",
    "# Randomly sample 5 combinations\n",
    "for _ in range(5):\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    for batch_size in batch_sizes:\n",
    "        optimizer = # Code Here\n",
    "        train_loader = # Code Here\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        print(f\"LR: {lr}, Batch size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_params = {'lr': lr, 'batch_size': batch_size}\n",
    "\n",
    "print(f\"Best Params (Random Search): {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ewVlxfjiv9L"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u46y5GicWU4Y"
   },
   "source": [
    "**Bayesian Optimization (Optuna):**\n",
    "\n",
    "Use optuna.create_study to dynamically suggest:\n",
    "* Learning rate: Range (0.0001, 0.1)\n",
    "* Hidden layer neurons: Range (32, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pX7C-hQWYzu"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest parameters\n",
    "    lr = # Code Here\n",
    "    neurons = # Code Here\n",
    "\n",
    "    # Modify model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(28*28, neurons),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(neurons, 10),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    optimizer = # Code Here\n",
    "    loss_function = # Code Here\n",
    "\n",
    "    # Train model\n",
    "    model.train()\n",
    "    num_epochs =\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            # Flatten images\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            # Forward pass\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Flatten images\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images)\n",
    "            loss = loss_function(predictions, labels)\n",
    "\n",
    "            # Append the total_loss\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)  # Average loss over all batches\n",
    "    return avg_loss  # Return loss for Optuna to minimize\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(f\"Best Params (Optuna): {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhLh1t7LYnmj"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAkXzAgHW9wQ"
   },
   "source": [
    "**Step5**: Evaluate and Compare the Model\n",
    "\n",
    "* Train the model using the best hyperparameters from each method (Grid Search, Random Search, Optuna).\n",
    "* num_epochs = 50\n",
    "* Evaluate all models on the test set.\n",
    "* Plot training/validation accuracy and loss for the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMRSKltYXPTD"
   },
   "outputs": [],
   "source": [
    "# Train model with best params and evaluate\n",
    "model = FashionNN()  # Re-initialize the model\n",
    "optimizer = # Code here\n",
    "train_loader = # Code here\n",
    "\n",
    "# Define loss function\n",
    "loss_function = # Code here\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50 # Re-train with best parameters\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "    for images, labels in train_loader:\n",
    "        # Clear previous gradients\n",
    "\n",
    "        predictions =                # Forward pass\n",
    "\n",
    "        loss =                       # Compute loss\n",
    "        # Backpropagation\n",
    "\n",
    "        # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for images, labels in test_loader:\n",
    "        predictions =            # Forward pass\n",
    "        loss =                   # Compute loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(predictions, 1)  # Get class with highest probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4LgAqQ1Yo7_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0sNkV7nYsZf"
   },
   "source": [
    "**Step6**: Visualize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1TNb1LaYwq1"
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "model.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "predictions = model(images).argmax(dim=1)\n",
    "\n",
    "# Plot 9 images\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Pred: {predictions[i]}, True: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
