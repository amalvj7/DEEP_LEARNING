{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpRkAlxQwQjy"
   },
   "source": [
    "# Problem Statement: **BONUS EXERCISE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yvpy57daLojm"
   },
   "source": [
    "Imports and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyYMNU-bmZXl"
   },
   "source": [
    "Problem 1: **Gradient Descent for Demand Forecasting at AtliQ**\n",
    "\n",
    "AtliQ wants to optimize the prediction of regional product demands using gradient descent.\n",
    "\n",
    "Assume the loss function is\n",
    "\n",
    "$$L(w)=(w‚àí4)^2$$\n",
    "\n",
    "where **w** is a weight parameter initialized at 0.\n",
    "\n",
    "**Write code to:**\n",
    "\n",
    "* Perform 10 iterations of gradient descent using a learning rate of 0.1.\n",
    "\n",
    "* Print the weight **w** at each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hOiJAjpnklhJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:\n",
    "### 1. Deriving the Gradient of the Loss Function\n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$$L(w) = (w - 4)^2$$\n",
    "\n",
    "Using the power rule:\n",
    "\n",
    "$$\\frac{d}{dw}(x^2) = 2x$$\n",
    "\n",
    "The derivative of $(w - 4)^2$ becomes:\n",
    "\n",
    "$$\\frac{d}{dw} (w - 4)^2 = 2(w - 4)$$\n",
    "\n",
    "Thus, the gradient of the loss function is:\n",
    "\n",
    "$$\\frac{dL}{dw} = 2(w - 4)$$\n",
    "\n",
    "### 2. Gradient Descent Weight Update Formula\n",
    "\n",
    "Gradient descent updates the weight using:\n",
    "\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{dL}{dw}$$\n",
    "\n",
    "Here,  **Œ∑** represents the **learning rate**, which controls how big a step we take during each update.\n",
    "\n",
    "Substituting the gradient expression:\n",
    "\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot 2(w_{\\text{old}} - 4)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bhYyNy9mndvd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: w = 0.8000\n",
      "Step 2: w = 1.4400\n",
      "Step 3: w = 1.9520\n",
      "Step 4: w = 2.3616\n",
      "Step 5: w = 2.6893\n",
      "Step 6: w = 2.9514\n",
      "Step 7: w = 3.1611\n",
      "Step 8: w = 3.3289\n",
      "Step 9: w = 3.4631\n",
      "Step 10: w = 3.5705\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "w = 0.0\n",
    "\n",
    "for i in range(10):\n",
    "    gradient = 2 * (w - 4)  # dL/dw\n",
    "    w = w - learning_rate * gradient  # Update rule\n",
    "    print(f\"Step {i+1}: w = {w:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xADdX0ULpDhP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40THnZYtoOkV"
   },
   "source": [
    "Problem 2: **Momentum for Contour Navigation in AtliQ's Supply Chain**\n",
    "\n",
    "AtliQ's supply chain optimization problem is represented by a contour map of a quadratic function:\n",
    "\n",
    "$$f(x,y)=x^2 +3y^2$$\n",
    "\n",
    "Write a code to implement gradient descent (5 iterations) with momentum to minimize this function.\n",
    "\n",
    "Use:\n",
    "* Initial point (x, y) = (2, 2)\n",
    "* Learning rate (Œ∑) = 0.1\n",
    "* Momentum Coefficient (Œ≤)) = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: \n",
    "### 1. Deriving the Gradient of the Function\n",
    "\n",
    "We want to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 + 3y^2\n",
    "$$\n",
    "\n",
    "Since this is a function of two variables, we compute the **partial derivatives** with respect to each variable.  \n",
    "We apply the power rule. For example, $\\frac{\\partial}{\\partial x}(x^2) = 2x$ and $\\frac{\\partial}{\\partial y}(y^2) = 2y$, treating the other variable as a constant.\n",
    "\n",
    "- Partial derivative with respect to \\(x\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 2x\n",
    "$$\n",
    "\n",
    "- Partial derivative with respect to \\(y\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 3 \\cdot 2y = 6y\n",
    "$$\n",
    "\n",
    "Thus, the gradients of $f(x, y)$ are simply $2x$ and $6y$.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Momentum Update Formula\n",
    "\n",
    "Momentum helps accelerate gradient descent by accumulating a velocity term.  \n",
    "The velocity updates are computed as:\n",
    "\n",
    "$$\n",
    "v_x = \\beta \\cdot v_x + (1 - \\beta)\\cdot\\,\\frac{\\partial f}{\\partial x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_y = \\beta \\cdot v_y + (1 - \\beta)\\cdot\\,\\frac{\\partial f}{\\partial y}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\beta$ is the **momentum coefficient**  \n",
    "- $\\eta$ is the **learning rate**.\n",
    "\n",
    "After computing the velocities, the parameters are updated using:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\eta \\cdot v_x\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{\\text{new}} = y_{\\text{old}} - \\eta \\cdot v_y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z2wHwhiHpCwf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: x = 1.9600, y = 1.8800\n",
      "Step 2: x = 1.8848, y = 1.6592\n",
      "Step 3: x = 1.7794, y = 1.3609\n",
      "Step 4: x = 1.6490, y = 1.0108\n",
      "Step 5: x = 1.4986, y = 0.6351\n"
     ]
    }
   ],
   "source": [
    "def gradient(x, y):\n",
    "    return 2*x, 6*y  # Gradients of f(x, y)\n",
    "\n",
    "x, y = 2.0, 2.0\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "vx, vy = 0.0, 0.0  # initialized velocity\n",
    "\n",
    "for i in range(5):\n",
    "    dx, dy = gradient(x, y)\n",
    "    # Momentum updates\n",
    "    vx = momentum * vx + (1 - momentum) * dx\n",
    "    vy = momentum * vy + (1 - momentum) * dy\n",
    "    # Update positions\n",
    "    x -= learning_rate * vx # use x += vx if learning_rate is applied inside the velocity update instead of (1 - momentum)\n",
    "    y -= learning_rate * vy # use y += vy if learning_rate is applied inside the velocity update instead of (1 - momentum)\n",
    "    print(f\"Step {i+1}: x = {x:.4f}, y = {y:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2tQRlsUqLvx"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJjaxm6kqNKE"
   },
   "source": [
    "Problem 3: **RMS Prop for AtliQ's Dynamic Pricing Optimization**\n",
    "\n",
    "AtliQ's AI model adjusts product prices dynamically. Implement the RMSProp optimizer for minimizing the function:\n",
    "\n",
    "$$f(w) = w^2 + 5$$\n",
    "\n",
    "Use:\n",
    "\n",
    "* Initial weight (ùë§) = 5.0\n",
    "* Learning rate (Œ∑) = 0.01\n",
    "* Momentum Coefficient(Œ≤)=0.9\n",
    "\n",
    "\n",
    "Run the optimization for 15 iterations and print the weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3:\n",
    "\n",
    "### 1. Deriving the Gradient of the Function\n",
    "\n",
    "We want to minimize the function:\n",
    "\n",
    "$$\n",
    "f(w) = w^2 + 5\n",
    "$$\n",
    "\n",
    "To compute the gradient, we differentiate using the power rule  $\\frac{d}{dx}(x^2) = 2x$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dw} = 2w\n",
    "$$\n",
    "\n",
    "Thus, the gradient of $f(w)$ is simply $2w$\n",
    "\n",
    "\n",
    "\n",
    "### 2. RMSProp Update Rule\n",
    "\n",
    "RMSProp helps stabilize gradient descent by scaling the learning rate using a **moving average of squared gradients**.\n",
    "\n",
    "The moving average of squared gradients is computed as:\n",
    "\n",
    "$$\n",
    "s_{new} = \\beta \\cdot s_{old} + (1 - \\beta)\\,\\cdot g^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g$ is the gradient, so here $g = 2w$\n",
    "- $\\beta$ is the **decay rate**\n",
    "- $s$ stores the **running average of squared gradients**\n",
    "- $\\epsilon$ is a small constant to avoid division by zero. A common choice is $10^{-8}$\n",
    "\n",
    "The RMSProp update rule is:\n",
    "\n",
    "$$\n",
    "w_{new}\n",
    "= w_{old} - \\frac{\\eta}{\\sqrt{s_{new} + \\epsilon}}\\,\\cdot g\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\eta$ is the **learning rate**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4GjcPpNKrwzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: w = 4.9684\n",
      "Step 2: w = 4.9455\n",
      "Step 3: w = 4.9264\n",
      "Step 4: w = 4.9094\n",
      "Step 5: w = 4.8939\n",
      "Step 6: w = 4.8794\n",
      "Step 7: w = 4.8657\n",
      "Step 8: w = 4.8526\n",
      "Step 9: w = 4.8400\n",
      "Step 10: w = 4.8277\n",
      "Step 11: w = 4.8158\n",
      "Step 12: w = 4.8041\n",
      "Step 13: w = 4.7927\n",
      "Step 14: w = 4.7814\n",
      "Step 15: w = 4.7704\n"
     ]
    }
   ],
   "source": [
    "def gradient(w):\n",
    "    return 2 * w   # Gradient of f(w)\n",
    "\n",
    "w = 5.0                     \n",
    "learning_rate = 0.01        \n",
    "beta = 0.9                  \n",
    "epsilon = 1e-8\n",
    "squared_gradient_average = 0.0   # initialized squared gradient average\n",
    "\n",
    "for i in range(15):\n",
    "    grad = gradient(w)\n",
    "    # RMSProp squared gradient average update\n",
    "    squared_gradient_average = beta * squared_gradient_average + (1 - beta) * (grad ** 2)\n",
    "    # RMSProp update rule\n",
    "    w = w - (learning_rate / ((squared_gradient_average + epsilon) ** 0.5)) * grad\n",
    "    print(f\"Step {i+1}: w = {w:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0tb736Ysolc"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87npAc9ysqOU"
   },
   "source": [
    "Problem 4: **Adam Optimizer for AtliQ AI Models**\n",
    "\n",
    "AtliQ is training an AI model to recommend warehouse restocking schedules. Use the Adam optimizer to minimize the function:\n",
    "\n",
    "$$f(x) = x^4 - 3x^3 + 2$$\n",
    "\n",
    "Write code to:\n",
    "\n",
    "* Initialize x = 3.0\n",
    "\n",
    "Run the optimizations for 19 iterations (starting from 1) with:\n",
    "* Learning rate (Œ∑) = 0.01\n",
    "* Momentum Coefficients: Œ≤1 = 0.9, Œ≤2 = 0.09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4:\n",
    "\n",
    "### 1. Deriving the Gradient of the Function\n",
    "\n",
    "We want to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x) = x^4 - 3x^3 + 2\n",
    "$$\n",
    "\n",
    "To compute the gradient, we differentiate each term using the power rule. The derivative becomes:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 4x^3 - 9x^2\n",
    "$$\n",
    "\n",
    "Thus, the gradient of $f(x)$ is simply $4x^3 - 9x^2$\n",
    "\n",
    "### 2. Adam Optimizer Update Rule\n",
    "\n",
    "Adam combines **Momentum** and **RMSProp** by maintaining:\n",
    "\n",
    "- a running average of gradients (first moment)\n",
    "- a running average of squared gradients (second moment)\n",
    "\n",
    "#### **Biased moment estimates**\n",
    "\n",
    "The first and second moment updates are:\n",
    "\n",
    "$$\n",
    "m_{new} = \\beta_1 \\cdot  m_{old} + (1 - \\beta_1) \\cdot  g\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_{new} = \\beta_2 \\cdot  v_{old} + (1 - \\beta_2) \\cdot  g^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g$ is the gradient, so here $g = 4x^3 - 9x^2$ \n",
    "- $\\beta_1$ controls the decay of the first moment  \n",
    "- $\\beta_2$ controls the decay of the second moment  \n",
    "- $m$ and $v$ store accumulated averages\n",
    "\n",
    "#### **Bias corrections**\n",
    "\n",
    "Because moments start at zero, Adam applies bias correction:\n",
    "\n",
    "$$\n",
    "\\hat{m} = \\frac{m}{1 - \\beta_1^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v} = \\frac{v}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "#### **Adam update rule**\n",
    "\n",
    "The parameter update is:\n",
    "\n",
    "$$\n",
    "x_{new}= x_{old} - \\frac{\\eta}{\\sqrt{\\hat{v}} + \\epsilon} \\,\\cdot \\hat{m}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\eta$ is the **learning rate**  \n",
    "- $\\epsilon$ is a small constant added to prevent division by zero  \n",
    "  (typically chosen as $10^{-8}$ in the Adam optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dQjK7sR4twzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: x = 2.9900\n",
      "Step 2: x = 2.9799\n",
      "Step 3: x = 2.9697\n",
      "Step 4: x = 2.9595\n",
      "Step 5: x = 2.9491\n",
      "Step 6: x = 2.9387\n",
      "Step 7: x = 2.9281\n",
      "Step 8: x = 2.9174\n",
      "Step 9: x = 2.9067\n",
      "Step 10: x = 2.8958\n",
      "Step 11: x = 2.8849\n",
      "Step 12: x = 2.8739\n",
      "Step 13: x = 2.8627\n",
      "Step 14: x = 2.8515\n",
      "Step 15: x = 2.8401\n",
      "Step 16: x = 2.8287\n",
      "Step 17: x = 2.8171\n",
      "Step 18: x = 2.8054\n",
      "Step 19: x = 2.7937\n"
     ]
    }
   ],
   "source": [
    "def gradient(x):\n",
    "    # Gradient of f(x) = x^4 - 3x^3 + 2\n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "x = 3.0\n",
    "learning_rate = 0.01\n",
    "beta1, beta2 = 0.9, 0.09\n",
    "epsilon = 1e-8\n",
    "first_moment, second_moment = 0.0, 0.0  # initialized first and second moment\n",
    "\n",
    "for t in range(1, 20): \n",
    "    grad = gradient(x)\n",
    "    m = beta1 * first_moment + (1 - beta1) * grad #update biased first moment\n",
    "    v = beta2 * second_moment + (1 - beta2) * (grad ** 2) # Update biased second moment\n",
    "    m_hat = m / (1 - beta1**t) #corrected first moment\n",
    "    v_hat = v / (1 - beta2**t) #corrected second moment\n",
    "    x = x - learning_rate * m_hat / ((v_hat ** 0.5) + epsilon) # update rule\n",
    "    first_moment, second_moment = m, v\n",
    "    print(f\"Step {t}: x = {x:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "953S3HD64qNm"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
